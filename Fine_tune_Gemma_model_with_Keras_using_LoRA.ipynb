{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQSteKhufNmJ"
      },
      "source": [
        "# Install and import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0mVZnM1e9aU",
        "outputId": "4db07490-3db4-401f-80aa-1bde130d57c4"
      },
      "outputs": [],
      "source": [
        "! nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbDXWrH3fM1U"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsWE7vZyfSiB",
        "outputId": "8a6307a5-9fab-43ee-ed3d-b54b87d9d943"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "! pip install -qq -U kaggle\n",
        "! pip install -qq -U keras-nlp\n",
        "! pip install -qq -U keras>=3\n",
        "! pip install -qq -U datasets\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rbk2tXafvEM"
      },
      "source": [
        "## Kaggle Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9WRZa7ofws-"
      },
      "outputs": [],
      "source": [
        "# copy kaggle.json to /root/.kaggle/ folder so that kaggle cli can access it.\n",
        "!mkdir /.kaggle\n",
        "!mv kaggle.json /.kaggle\n",
        "!mv /.kaggle /root/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNt6xgdwfjIm"
      },
      "source": [
        "## Select a Backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVvEz1W-flOw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyhQFvCpf4Kc"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Byh8U-bf6a0"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBh3VAiIf-xH"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw4cFuKtgAPx"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "data = []\n",
        "\n",
        "with open(\"/content/databricks-dolly-15k.jsonl\") as file:\n",
        "  for line in file:\n",
        "    features = json.loads(line)\n",
        "\n",
        "    #Filter out examples with context, to keep it simple.\n",
        "    if features[\"context\"] == \"\":\n",
        "      continue\n",
        "\n",
        "    # Format the entire example as a single string.\n",
        "    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "    data.append(template.format(**features))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjW9cuwqhx2c"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "QtjNJHXEhy9S",
        "outputId": "91cb9dda-bcd7-4a05-8bd5-d631cf9afc67"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAXfT79gidka"
      },
      "source": [
        "# Inference before fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9TiXN-Qih_Q",
        "outputId": "0ef6ab62-d0f4-4555-a58f-929726287673"
      },
      "outputs": [],
      "source": [
        "## Brazil Trip Prompt\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction=\"What should I do on a trip to Brazil ?\",\n",
        "    response=\"\"\n",
        ")\n",
        "\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdGQzJ_xjCK8",
        "outputId": "284f2275-a5e2-40db-9070-8de8423efa34"
      },
      "outputs": [],
      "source": [
        "#ELI5 Photosynthesis Prompt\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction=\"Explatin the process of photosynthesis in a way that a child could understand.\",\n",
        "    response=\"\"\n",
        ")\n",
        "\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7q-S_FhqA0t"
      },
      "source": [
        "# LoRA Fine-Turing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fShoeyWqEm1"
      },
      "source": [
        "To get better responses from the model, fine-tune the model with Low Rank Adaptation (LoRA) using the Databricks Dolly 15k dataset.\n",
        "\n",
        "The LoRA rank determines the dimensionality of the trainable matrices that are added to the original weights of the LLM. It controls the expressiveness and precision of the fine-tuning adjustments.\n",
        "\n",
        "A higher rank means more detailed changes are possible, but also means more trainable parameters. A lower rank means less computational overhead, but potentially less precise adaptation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Ka031Q7_qHNG",
        "outputId": "f53271fb-737d-4dfb-9ae8-b8667ed81d30"
      },
      "outputs": [],
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to 4.\n",
        "gemma_lm.backbone.enable_lora(rank=4)\n",
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIBPSTEqqYYD"
      },
      "source": [
        "Note that enabling LoRA reduces the number of trainable parameters significantly (from 2.5 billion to 1.3 million)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxkID_eaqaOT"
      },
      "outputs": [],
      "source": [
        "# Limit the input sequence length to 512 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 512\n",
        "\n",
        "# Use AdamW (a common optimizer for tranformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "  learning_rate=5e-4,\n",
        "  weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  optimizer=optimizer,\n",
        "  weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "gemma_lm.fit(data, epochs=1, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference after fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Brazil Trip Prompt\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction=\"What should I do on a trip to Brazil ?\",\n",
        "    response=\"\"\n",
        ")\n",
        "\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ELI5 Photosynthesis Prompt\n",
        "\n",
        "prompt = template.format(\n",
        "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
        "    response=\"\"\n",
        ")\n",
        "\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
